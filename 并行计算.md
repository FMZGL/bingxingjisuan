### 1. PVP（Parallel Vector Processor，并行向量处理机）

- **核心特点**：由少数几个（通常 1-8 个）高性能向量处理器组成，共享一个大容量内存和 I/O 系统，通过高带宽总线连接。
- **通俗理解**：像几个 “超级计算器” 共用一个大仓库（内存），每个计算器擅长处理成组的数字（向量），比如同时计算 1000 对数字的加减。
- **典型场景**：早期超级计算机（如 Cray 系列），适合处理气象模拟、流体力学等需要大量向量运算的任务。
- **缺点**：成本高，扩展性差（很难超过 8 个处理器）。

### 2. SMP（Symmetric Multi-Processor，对称多处理器）

- **核心特点**：多个相同的处理器（如 4 核、8 核 CPU）通过共享总线连接到**同一块内存**，所有处理器地位平等（对称），都能直接访问内存和 I/O 设备。
- **通俗理解**：一群能力相同的人围坐在一张大桌子旁，桌上的文件（数据）大家都能看、能改，谁要用水笔（I/O 设备）直接拿就行，没有 “领导” 和 “下属” 之分。
- **我们最熟悉的模型**：日常用的多核电脑（如笔记本、台式机）都是 SMP 结构 ——4 核 CPU 就是 4 个处理器共享同一块内存。
- **优点**：编程简单（不用考虑数据分配），通信快（直接读内存）。
- **缺点**：处理器数量有限（一般不超过 64 个），否则总线会成为瓶颈（大家抢着用总线，反而变慢）。

### 3. MPP（Massively Parallel Processor，大规模并行处理机）

- **核心特点**：由成百上千个独立节点（每个节点是一个处理器 + 自己的私有内存）组成，节点之间通过高速网络（如 InfiniBand）连接，**没有共享内存**。
- **通俗理解**：成千上万的人分散在不同房间，每人有自己的笔记本（私有内存），要交换数据只能靠打电话（网络通信），谁也不能直接看别人的笔记本。
- **典型场景**：顶级超级计算机（如 “天河一号”“神威・太湖之光”），适合处理超大规模任务（如全球气候模拟、基因测序）。
- **优点**：扩展性极强（能轻松做到几万甚至几十万处理器），总计算能力惊人。
- **缺点**：编程复杂（需要明确数据如何在节点间传递），通信延迟比 SMP 高。

### 4. DSM（Distributed Shared Memory，分布式共享内存）

- **核心特点**：物理上是多个节点各有私有内存（类似 MPP），但通过软件或硬件模拟出 “共享内存” 的效果 —— 每个节点可以 “假装” 直接访问其他节点的内存，不用显式发消息。
- **通俗理解**：还是分散在不同房间的人，每人有自己的笔记本，但通过一个 “云同步” 工具，让大家觉得在编辑同一个文档（实际是后台自动同步），不用每次手动传文件。
- **优点**：兼顾 MPP 的扩展性和 SMP 的编程简单性（程序员不用手动管理数据传输）。
- **缺点**：同步和一致性维护复杂，“假装共享” 的背后有额外开销（比如网络传输延迟）。

### 5. COW（Cluster of Workstations，工作站集群）

- **核心特点**：用普通 PC 或工作站（比如几十台台式机）通过以太网连接成集群，节点之间地位平等，通常通过软件（如 MPI、Hadoop）实现协作。
- **通俗理解**：把办公室里的几十台普通电脑用网线连起来，安装一套 “协作软件”，让它们合力完成一个大任务（比如同时渲染一部电影的不同镜头）。
- **优点**：成本极低（用现成设备），灵活（随时加机器或减机器）。
- **缺点**：性能不如专用 MPP（普通网络速度慢），适合预算有限的场景（如高校科研、中小企业数据处理）。

### 一句话总结区别

- **SMP**：少数处理器共享内存（“同桌协作”）
- **MPP**：海量处理器各有内存，靠网络通信（“异地大军”）
- **DSM**：物理分散，假装共享内存（“异地协作但像同桌”）
- **PVP**：专用向量处理器，适合特定运算（“专业团队”）
- **COW**：普通电脑凑成的集群（“平民军团”）







# 二、并行计算机访存模式

要理解并行计算机的 “访存模型”，可以先抓住核心：它本质是 “多个计算单元（比如 CPU 核心、节点）怎么去拿数据” 的规则。就像一群人去仓库取货，不同的访存模型，就是不同的 “取货流程”—— 有的直接拿，有的要登记，有的得按顺序拿。下面用 “仓库取货” 的生活场景，拆解 4 种最核心的访存模型，保证一听就懂：

### 1. UMA（均匀存储访问模型）：“所有人到同一个仓库取货，速度一样”

#### 核心逻辑：

所有计算单元（比如 CPU 核心）共享**同一块内存**（相当于 “一个公共仓库”），而且不管哪个计算单元去拿数据，从内存读 / 写的速度都一样（“均匀” 就是这个意思）。

#### 生活类比：

公司所有人都去同一个中央仓库取文件，仓库只有一个出入口，不管是老板、员工，从门口走到货架拿文件的时间都一样，没有 “特殊通道”。

#### 关键特点：

- **优点**：规则简单 —— 不用管 “该去哪个仓库”，所有人盯准一个地方就行，编程也容易（不用考虑不同内存的速度差异）。
- **缺点**：“仓库门口会堵车”—— 如果计算单元太多（比如几十上百个核心），大家都挤着去同一个内存拿数据，会出现 “内存访问冲突”，反而变慢（相当于仓库门口排队）。

#### 常见场景：

我们日常用的多核电脑（比如 4 核、8 核 CPU）就是 UMA—— 所有核心共享同一条内存条，访问速度没有差别；早期的小型并行机也多是这种模型。

### 2. NUMA（非均匀存储访问模型）：“每人有自己的小仓库，也能去别人的小仓库，速度不一样”

#### 核心逻辑：

把内存拆成好几块，每个 “计算单元组”（比如 2 个 CPU 核心算一组）配一块 “本地内存”（相当于 “个人小仓库”）；同时，所有计算单元也能访问其他组的 “远程内存”（别人的小仓库）。但关键是：访问自己的本地内存很快，访问远程内存很慢（“非均匀” 就是速度不一样）。

#### 生活类比：

公司按部门分了小仓库 —— 销售部有销售仓库，技术部有技术仓库。销售拿自己仓库的文件只要 1 分钟，但要拿技术仓库的文件，得先联系技术部登记，再跑过去，要 5 分钟。

#### 关键特点：

- **优点**：“减少堵车”—— 大部分时候，计算单元只拿自己本地内存的数据，不用挤公共仓库；而且内存总容量大（多个小仓库加起来），能支持更多计算单元。
- **缺点**：“得记清仓库位置”—— 编程时要尽量让计算单元用自己的本地内存（不然远程访问太慢，拖慢整体速度），相当于 “取货前先想清楚：自己仓库有没有，没有再去别人那”。

#### 常见场景：

中大型服务器（比如用于数据库、云计算的 2 路 / 4 路 CPU 服务器）—— 比如 2 个 CPU 芯片，每个 CPU 带自己的内存条（本地内存），但两个 CPU 也能互相访问对方的内存条（远程内存）。

### 3. NORMA（非远程存储访问模型）：“每人只有自己的小仓库，不能去别人的，要数据只能让别人送过来”

#### 核心逻辑：

每个计算单元都有**完全属于自己的私有内存**（自己的小仓库），而且不能直接去访问其他计算单元的内存 —— 如果需要别人的数据，必须通过 “网络发消息”（比如 “请把你的数据传我一份”），让对方把数据 “送过来”，自己不能主动去拿。

#### 生活类比：

异地办公的团队，每人家里只有自己的文件柜（私有内存）。北京的同事要上海同事的文件，不能直接去上海开文件柜，只能发消息让上海同事把文件拍照发过来。

#### 关键特点：

- **优点**：“绝对不堵车”—— 每个计算单元的内存只有自己用，不会有冲突；而且可以无限加计算单元（只要网络能连），扩展性极强（相当于可以加无数个异地同事，每人一个文件柜）。
- **缺点**：“取货效率低”—— 要拿别人的数据，得等对方传过来，尤其是数据量大时（比如传一个 1G 的文件），会花很多时间（相当于等快递）；编程也麻烦（得明确 “谁要什么数据”“什么时候传”）。

#### 常见场景：

大规模并行机、超级计算机（比如 “天河”“神威”）、大数据集群（比如多台服务器跑 Spark 任务）—— 每台服务器就是一个计算单元，有自己的内存，靠高速网络互相传数据。

### 4. CC-NUMA（高速缓存一致性非均匀存储访问模型）：“每人有小仓库 + 随身抽屉，抽屉里的东西要保持一致”

#### 核心逻辑：

在 NUMA 的基础上，给每个计算单元加了 “高速缓存”（相当于 “随身抽屉”）—— 计算单元常用的数据会存在抽屉里，拿起来更快；同时，通过特殊规则保证：**所有计算单元的 “抽屉” 里，同一份数据的内容是一样的**（比如 A 的抽屉里有 “文件 X”，B 的抽屉里也有 “文件 X”，如果 A 改了自己抽屉里的 X，B 的抽屉里的 X 会自动同步更新，不会出现 “A 改了 X，B 还拿着旧 X” 的情况，这就是 “高速缓存一致性”）。

#### 生活类比：

还是部门分小仓库，每个人除了能去部门仓库，还随身带个抽屉（高速缓存），常用的文件放抽屉里。公司有个规则：如果 A 改了自己抽屉里的 “客户名单”，会立刻通知所有人 “客户名单更新了”，大家抽屉里的旧名单会自动换成新的，保证所有人手里的名单都一样。

#### 关键特点：

- **优点**：“又快又准”—— 随身抽屉（高速缓存）让常用数据访问更快，小仓库（本地内存）减少冲突，还能保证数据一致（不会用错旧数据）。
- **缺点**：“规则复杂”—— 要维护所有抽屉的数据一致，需要额外的硬件 / 软件逻辑（相当于公司要专门安排人负责 “同步名单”），成本更高。

#### 常见场景：

高端服务器、高性能计算节点（比如用于 AI 训练、科学计算的服务器）—— 比如 4 个 CPU 组成的节点，每个 CPU 带本地内存和高速缓存，同时通过 “缓存一致性协议”（比如 Intel 的 QPI 协议）保证缓存数据一致。

# 2.1.2、静态互连网络

假设并行计算机是一个 “大办公室”，里面的每台 “小电脑”（专业叫 “处理单元 / PE”）就是一个 “员工”。员工之间要传递文件（数据）才能协同干活，而 “静态互联网络” 就是办公室里**固定不变的 “传文件通道”**—— 比如预先铺好的专线、固定的桌面传递路线，一旦建好就不能随便改，员工只能沿着这些通道传东西，不能临时加新通道或改路线。

简单说，它的核心就是 “**通道固定，不能动态调整**”，所有处理单元之间的连接关系是提前设计好的（比如用硬件电路焊死），工作时不会变。

### 1. 一维线性阵列（Linear Array）

- **结构**：所有节点按直线排列，每个中间节点仅连接左右相邻节点，两端节点只连接一个邻居（如 1-2-3-…-N）。
- **节点度**：两端节点度为 1，中间节点度为 2，平均节点度≈2。
- **网络直径**：N 个节点时，直径为 N-1（如 1 到 N 需经过 N-1 步）。
- **对剖宽度**：1（从任意位置切开直线，只需切断 1 条边）。

**特点**：结构最简单，成本极低，但直径随节点数线性增长，通信效率差，适合小规模、数据按顺序处理的场景（如流水线计算）。

### 2. 二维网孔（2D Mesh）

- **结构**：节点排列成 M×N 的网格，每个内部节点连接上下左右 4 个邻居，边缘节点连接 2-3 个邻居（若边缘不相连则为 “非环绕网孔”；若每行首尾、每列首尾相连则为 “环面网孔（Torus）”）。
- **节点度**：非环绕网孔中，内部节点度为 4，边缘节点度为 2-3，平均节点度≈4；环面网孔中所有节点度均为 4。
- 网络直径
  - 非环绕网孔（√N×√N 规模）：直径≈2√N（对角线节点需横向 + 纵向各√N 步）。
  - 环面网孔：直径≈√N（可双向绕路，缩短一半距离）。
- 对剖宽度
  - 非环绕网孔：√N（沿中线切开，需切断√N 条边）。
  - 环面网孔：√N（同样沿中线切开，但因环绕连接，对剖宽度不变）。

**特点**：节点度固定（4），直径随节点数平方根增长，平衡性好，是大型并行机常用结构（如天河超级计算机）。

### 3. 树（Tree）

- **结构**：以根节点为顶端，每层节点连接下一层节点，形成分层分支结构（如二叉树：根节点连接 2 个子节点，每个子节点再连接 2 个孙节点，直到叶节点）。
- **节点度**：根节点和内部节点度为 k（k 叉树，通常 k=2），叶节点度为 1，平均节点度≈2。
- **网络直径**：N 个节点的二叉树，直径≈2log₂N（从最左叶节点到最右叶节点需经过根节点，路径长度为 2× 层数）。
- **对剖宽度**：1（切断根节点与下一层的连接，即可将树分为两部分）。

**特点**：直径较小（对数增长），但对剖宽度极小（瓶颈在根节点），导致数据传输带宽受限，适合层级化控制场景，不适合大规模数据并行。

### 4. 超立方（Hypercube）

- **结构**：n 维超立方由 2ⁿ个节点组成，每个节点用 n 位二进制编号，编号仅差 1 位的节点直接相连（如 3 维超立方：000 与 001、010、100 相连，共 8 个节点）。
- **节点度**：n（n 维超立方中每个节点连接 n 个邻居）。
- **网络直径**：n（任意两节点的二进制编号差 k 位，最短路径为 k 步，最大值为 n）。
- **对剖宽度**：2ⁿ⁻¹（将节点按最高位 0/1 分为两组，每组 2ⁿ⁻¹ 个节点，需切断 2ⁿ⁻¹ 条边）。

**特点**：节点度、直径均随节点数对数增长（因 N=2ⁿ，n=log₂N），对剖宽度大（接近 N/2），通信效率极高，适合大规模并行计算，但结构复杂，扩展时需按 2ⁿ倍增加节点。

### 5. 嵌入（Embedding）

- **说明**：嵌入不是独立拓扑，而是指将一种网络结构 “映射” 到另一种网络中（如将树结构嵌入到超立方中），使原网络的节点和连接关系在目标网络中保持等效。

- 指标分析

  ：嵌入的性能取决于目标网络和映射方式，例如：

  - 将树嵌入超立方：节点度由树的 k 变为超立方的 n，直径可能因映射优化而小于原树，对剖宽度取决于超立方的特性。
  - 若嵌入不合理（如将超立方嵌入线性阵列），可能导致直径大幅增加（失去原网络优势）。

**作用**：解决不同拓扑网络的兼容问题，使算法能在不同架构的并行机上运行。



# 2.1.3 动态互连网络

动态互联网络和静态互联网络的核心区别在于：**连接关系可以灵活变化**。就像现实中的交通系统，静态网络是 “固定轨道”（比如火车只能沿铁轨跑），而动态网络是 “有调度员的公路网”—— 可以根据实时需求（比如哪条路堵了）临时改变数据的传输路线。

动态网络里有 “中间调度设备”（类似交通信号灯或调度中心），能根据数据的目的地，实时选择最优路径，不用像静态网络那样只能走固定路线。下面用生活化的例子介绍三种最常见的动态网络：

### 1. 总线（Bus）：就像 “办公室里的一条公共电话线”

- **结构**：所有处理单元（比如 10 台电脑）都连到同一条 “总线”（可以理解为一根共享的数据线）上，就像多个工位共用一条电话线。
- **工作方式**：某台电脑要发数据时，先 “抢总线”（类似打电话前先拿起听筒听有没有占线），抢到后才能发，其他电脑只能等着。数据发出去后，所有电脑都会收到，但只有目标电脑会 “接电话”， others 自动忽略。
- **优点**：结构超简单（一条线连所有设备），成本极低，适合设备少的场景（比如早期的家用电脑扩展槽）。
- **缺点**：“一拥而上” 会堵车 —— 设备越多，抢总线的冲突越频繁，速度越慢。就像 100 个人抢用一条电话线，大部分时间都在等。

### 2. 交叉开关（Crossbar Switch）：像 “全互通的十字路口”

- **结构**：假设有 N 个发送端和 N 个接收端，中间有一个 “交叉开关矩阵”—— 每个发送端和每个接收端之间都有一个独立的 “开关”（类似十字路口的每个方向都有单独的红绿灯）。比如 4 台发送机和 4 台接收机，就有 4×4=16 个开关。
- **工作方式**：发送数据时，直接打开对应路径的开关。比如 “发送机 1→接收机 3”，就把它们之间的开关打开，其他路径互不干扰。多个不同路径可以同时传输（只要不冲突），比如同时打开 “1→3” 和 “2→4” 的开关。
- **优点**：速度极快，没有冲突（只要路径不重复），就像每个方向的车都有专属车道，互不影响。
- **缺点**：成本太高 —— 开关数量是 N²，当设备多到 100 个时，就需要 10000 个开关，硬件复杂度爆炸，只适合小规模场景（比如高端服务器内部）。

### 3. 多级互联网络（Multistage Interconnection Network）：像 “多层换乘的地铁网”

- **结构**：把多个 “小交叉开关” 按层级连接起来，形成多级结构。比如最经典的 “Omega 网络”，有 3 级开关，每级有 4 个小开关，整体能连接 8 个发送端和 8 个接收端（类似地铁 1 号线→换乘 2 号线→换乘 3 号线到达目的地）。
- **工作方式**：数据从发送端出发，经过多级开关 “换乘”，最终到达接收端。比如 “发送端 5→接收端 3”，可能先经过第 1 级开关 A，再经第 2 级开关 B，最后经第 3 级开关 C 到达，路径可以动态选择。
- **优点**：平衡了成本和性能 —— 开关数量是 N×logN（比交叉开关的 N² 少得多），但能支持较多设备（比如几百到几千个节点），而且可以同时传输多条不冲突的路径。
- **缺点**：结构比总线复杂，可能出现 “路径冲突”（比如两条数据想走同一段线路），需要调度算法解决。

# 2.2.1 选路方法

并行计算机的 “选路方法”，简单说就是**数据在不同处理单元（节点）之间传输时，如何选择路径的规则**。就像快递员送包裹需要规划路线（走哪条街、怎么转弯），数据从 “起点节点” 到 “终点节点” 也需要一套固定规则来确定路径，避免混乱或冲突。

选路方法的核心是解决两个问题：① 怎么找到从起点到终点的路径；② 怎么避免不同数据在传输时 “撞车”（比如同时争抢同一条线路）。下面介绍两种经典的选路方法：

### 1. X-Y 选路法：“先横后竖” 的网格导航

X-Y 选路法专门用于**二维网孔（或环面网孔）结构**的并行计算机（节点排列成网格，类似棋盘坐标），规则像城市导航里的 “先沿 X 轴走，再沿 Y 轴走”。

#### 举个例子：

假设节点按网格坐标编号，比如起点是 (2,1)，终点是 (5,4)（前一个数是 X 坐标，后一个是 Y 坐标）：

- **第一步（X 方向）**：先沿水平方向（X 轴）移动，从 X=2 走到 X=5（不管 Y 坐标），比如路径是 (2,1)→(3,1)→(4,1)→(5,1)；
- **第二步（Y 方向）**：再沿垂直方向（Y 轴）移动，从 Y=1 走到 Y=4（X 坐标固定为 5），路径是 (5,1)→(5,2)→(5,3)→(5,4)。

#### 特点：

- **规则简单**：就像 “先横着走到目标列，再竖着走到目标行”，不会走回头路；
- **避免冲突**：因为所有数据都按 “先 X 后 Y” 的顺序走，方向统一，不容易在路口 “撞车”；
- **适用场景**：所有二维网孔结构的并行机（比如很多超级计算机用的 Torus 结构），是网格网络中最常用的选路法。

### 2. E - 立方选路法：“按位翻牌” 的超立方导航

E - 立方选路法专门用于**超立方结构**的并行计算机（节点按二进制编号，比如 3 维超立方的节点编号是 000、001、010…111），规则类似 “按二进制位一步步修正”。

#### 举个例子：

3 维超立方中，起点是 001（二进制），终点是 110：

- 先看二进制的最高位（第 3 位）：起点是 0，终点是 1→不匹配，所以先把这一位 “翻成 1”，路径 001→101；
- 再看中间位（第 2 位）：当前是 0，终点是 1→不匹配，翻成 1，路径 101→111；
- 最后看最低位（第 1 位）：当前是 1，终点是 0→不匹配，翻成 0，路径 111→110。

每一步只改一个二进制位，最终从起点 “翻” 到终点。

#### 特点：

- **按位导航**：像玩 “翻牌游戏”，每次只修正一个二进制位，保证每步都向终点靠近；
- **路径唯一**：只要按 “从高位到低位”（或固定顺序）翻位，从起点到终点的路径是唯一的，避免迷路；
- **适用场景**：所有超立方结构的并行机，充分利用超立方 “节点间距离短” 的优势（n 维超立方最多只需 n 步）。



# 2.2.2 开关技术

在并行计算机的互联网络中，“开关技术” 指的是**数据在节点或交换机之间传输时，如何处理、转发消息的规则和方式**。就像快递站处理包裹的流程（比如怎么打包、怎么分拣、怎么传递），开关技术决定了数据 “消息” 在网络中如何被处理和转发，核心是提高传输效率、减少延迟。

下面用 “快递运输” 的例子，通俗解释开关技术中的三个关键概念：

### 1. 消息格式：数据的 “打包方式”

消息格式就是**规定数据在传输时的 “包装结构”**，就像快递包裹必须有 “面单（收件人信息）+ 货物（实际内容）” 一样，网络中的消息也需要固定格式，让接收方知道如何处理。

通常包含三部分：

- **头部（Header）**：相当于快递面单，记录关键信息 —— 比如消息要发到哪个节点（目的地地址）、消息长度、优先级等，是交换机或节点判断 “怎么转发” 的依据。
- **数据体（Data）**：相当于包裹里的货物，是真正要传输的内容（比如计算数据、指令等）。
- **尾部（Trailer）**：相当于快递单的备注，可能包含校验信息（比如检查数据是否传输中出错）、消息结束标记等。

**作用**：统一格式让所有节点和交换机 “看得懂” 消息，知道该往哪发、怎么处理，避免混乱。

### 2. 存储转发（Store-and-Forward，SF）选路：“先存全，再转发”

存储转发是最经典的消息转发方式，规则类似 **“快递站必须收到完整包裹后，才会继续往下一站发”**。

#### 过程：

- 消息从起点出发，到达第一个交换机（或中间节点）时，交换机先把**整个消息**（包括头部、数据体、尾部）完整接收并存储起来；
- 确认收到完整消息后，交换机根据消息头部的目的地信息，决定下一站该发往哪里；
- 再把整个消息转发到下一个交换机，重复这个过程，直到到达终点。

#### 例子：

就像你寄一个大包裹到外地：

- 本地快递点必须先收到你的完整包裹（拆开检查、扫码存档），才会装上运往省会的车；
- 省会快递点收到完整包裹后，再转发到目的地城市的快递点，最后送到收件人手里。

#### 特点：

- 优点：安全可靠，中间节点能检查消息是否完整、有没有错误，错了可以要求重发；
- 缺点：延迟高，必须等整个消息都到了才能转发，尤其对大消息来说，等待时间长。

### 3. 切通（Cut-Through，CT）选路：“边收边发，不等全到”

切通选路是为了减少延迟设计的，规则类似 **“快递站只要看到面单，就先把包裹往目的地方向发，不用等整个包裹收完”**。

#### 过程：

- 消息从起点出发，先传输 “头部” 到第一个交换机；
- 交换机收到头部后，立刻解析出目的地信息，确定下一站路径；
- 不等后面的数据体和尾部完全到达，就开始把已经收到的部分（包括头部和陆续到达的数据体）转发到下一个交换机；
- 整个过程中，消息像 “流水” 一样在网络中传递，前一段还在传输，后一段已经开始转发。

#### 例子：

类似你用即时通讯发一段长文字：

- 你刚打完前半句 “我明天要...”，消息就已经开始往服务器传了；
- 服务器收到前半句，不等你打完后半句，就先转发给接收方，实现 “边发边传”。

#### 特点：

- 优点：延迟低，尤其对长消息来说，比存储转发快很多（不用等完整接收）；
- 缺点：如果中间发现路径堵塞或错误，已经发出去的部分可能白费，需要重新传输，可靠性略低于存储转发。



# 2.3 单一信包一到一传输

“单一信包一到一传输” 是并行计算机网络中最基础的一种数据传输方式，可以拆成三个关键词来理解：

- **“单一信包”**：指数据被打包成一个完整的 “信息包”（类似一个快递包裹），而不是分成多个小包传输。
- **“一到一”**：指数据传输的关系是 “一个发送节点” 到 “一个接收节点”，就像 “一对一” 寄快递，不是 “一个寄给多个”（一对多），也不是 “多个寄给一个”（多对一）。
- **“传输”**：就是这个信包从发送节点通过网络传到接收节点的过程。

举个生活例子：就像你给朋友寄一个完整的包裹，包裹没被拆开（单一信包），而且是你一个人寄给这一个朋友（一到一），快递小哥把包裹从你家送到朋友家（传输）。

这种方式的特点是：

- 简单直接，不需要复杂的拆分或分发逻辑；

- 是并行计算机中最基本的通信方式，其他复杂传输（比如一对多、多对多）往往都是基于它组合实现的；

- 适合传输少量数据或独立的指令，比如两个处理单元之间交换计算结果。

  

# 2.4.1 使用SF进行一到多播送

在环、环绕网孔和超立方等静态互连网络中，使用**存储转发（SF）技术**实现 “一到多传输”（一个源节点向多个目标节点发送数据），核心思路是：源节点将数据打包成消息，通过存储转发机制，按特定规则依次或并行地将消息传递到各个目标节点。具体网络的拓扑结构不同，具体路径选择和转发策略也不同，下面分别说明：

### 1. 环（Ring）网络上的 SF 一到多传输

**环网络特点**：节点连成闭合回路，每个节点只与左右两个邻居直接相连（如 0-1-2-3-0）。

**SF 一到多传输过程**：
假设源节点是 0，要向节点 2、3 发送数据：

- 源节点 0 先将数据打包成消息，消息头部包含所有目标节点地址（或按顺序标记下一个目标）。
- **第一步**：0 将消息发送给左邻居 1（或右邻居 3，根据最近路径选择）。节点 1 采用 SF 方式，先完整接收消息并存储，解析头部后发现需要继续转发。
- **第二步**：若目标包括 3，节点 1 将消息转发给 2；节点 2 接收并存储后，若自身是目标则留下数据，再转发给 3；节点 3 接收后，若自身是目标则留下数据，继续转发（因环是闭合的，需设置 “终止条件” 避免消息无限循环，比如标记已访问节点）。
- **优化策略**：采用 “令牌传递” 或 “广播令牌”，源节点发送的消息附带 “可复制标记”，每个中间节点收到后，若自身是目标则复制数据，再按环方向转发给下一个节点，直到所有目标都收到。

**特点**：消息沿环单向或双向流动，靠中间节点存储转发和复制实现多目标传输，延迟随环长度和目标数量增加而增加。

### 2. 环绕网孔（Torus，二维环面）上的 SF 一到多传输

**环绕网孔特点**：节点排列成二维网格，且每行 / 列首尾相连（如 3×3 环面中，(0,2) 右邻居是 (0,0)，(2,1) 下邻居是 (0,1)），每个节点有 4 个邻居（上下左右）。

**SF 一到多传输过程**：
假设源节点是 (0,0)，要向 (1,2)、(2,1) 发送数据：

- 源节点 (0,0) 将数据打包，消息头部包含所有目标坐标，采用 X-Y 选路的扩展策略（先按 X 方向转发，再按 Y 方向，因是环绕结构可双向选路）。
- 向 (1,2) 传输
  - (0,0)→(1,0)（X 方向 + 1），节点 (1,0) 完整接收消息（SF 机制），解析到目标 (1,2) 需 Y 方向 + 2，转发给 (1,1)；
  - (1,1) 接收存储后，转发给 (1,2)，目标节点接收并存储数据。
- 向 (2,1) 传输
  - 源节点 (0,0) 可并行发送另一份消息（或同一消息标记多个目标），经 Y 方向 + 1 到 (0,1)，节点 (0,1) 存储后，沿 X 方向 + 2（因环绕，X=0→2 只需 + 2 步）转发到 (2,1)，目标节点接收。

**特点**：利用二维环绕结构的多路径特性，可并行向不同区域的目标转发消息，中间节点通过解析头部坐标决定转发方向，SF 机制保证消息完整后再传递，可靠性高。

### 3. 超立方（Hypercube）网络上的 SF 一到多传输

**超立方特点**：n 维超立方有 2ⁿ个节点，每个节点用 n 位二进制编号，仅相差 1 位的节点直接相连（如 3 维超立方中，001 与 000、011、101 相连）。

**SF 一到多传输过程**：
假设 3 维超立方中，源节点是 000，要向 010、101 发送数据：

- 源节点 000 将数据打包，消息头部包含目标节点的二进制编号，采用 E - 立方选路的扩展策略（按位修正，支持多目标标记）。
- 向 010 传输
  - 000 与 010 的二进制差异在第 2 位（从左数），000 先将消息转发给 010（直接相连），目标节点完整接收（SF 机制）后存储数据。
- 向 101 传输
  - 000 与 101 的差异在第 1 位和第 3 位，000 先转发给 100（修正第 1 位），节点 100 完整接收消息后，解析到还需修正第 3 位，转发给 101，目标节点接收存储。

**优化策略**：若目标节点较多，可利用超立方的 “广播特性”—— 源节点先将消息发给所有直接相连的邻居（如 000 发给 001、010、100），每个邻居接收后（SF 机制），再转发给各自的邻居（排除源节点），以此类推，实现消息在超立方中 “扩散式” 传播，直到所有目标节点收到。

**特点**：超立方节点间距离短（最多 n 步），SF 转发时中间节点只需处理 1 位差异，效率高，适合快速向多个分散目标传输数据。



# 2.4.2使用CT进行一到多播送

在环、网孔和超立方网络中，使用**切通（CT）技术**实现 “一到多传输”（一个源节点向多个目标节点发送数据），核心思路是：源节点无需等待整个消息完全生成，只要确定头部信息（包含目标地址），就可立即开始传输，中间节点收到头部后也会边接收消息边转发，大幅降低延迟。具体实现因网络拓扑不同而有所差异，下面分别说明：

### 1. 环（Ring）网络上的 CT 一到多传输

**环网络特点**：节点连成闭合回路，每个节点只与左右邻居相连（如 0-1-2-3-0），数据只能沿环单向或双向流动。

**CT 一到多传输过程**：
假设源节点 0 要向节点 2、3 发送数据：

- 源节点 0 生成消息头部（包含目标节点 2、3 的标识），无需等待完整数据体生成，就立即将头部发送给右邻居 1（CT 的 “边生成边传” 特性）。
- 节点 1 收到头部后，不等完整消息到达，立即解析出目标包含 2、3，判断需要继续向右转发，于是边接收后续数据体，边将已收到的部分转发给节点 2。
- 节点 2 收到头部和部分数据后，发现自身是目标之一，立即复制已收到的数据（同时继续接收剩余部分），并继续向右转发给节点 3。
- 节点 3 收到后，同样复制数据（自身是目标），并继续转发（为避免消息在环中无限循环，头部会包含 “生命周期” 标记，每经过一个节点减 1，减到 0 则停止转发）。

**特点**：消息像 “流水” 一样沿环流动，中间节点 “边收边传”，比存储转发（SF）延迟更低；但如果某个目标节点后面还有其他目标，必须按顺序传递，无法跳步。

### 2. 网孔（Mesh）网络上的 CT 一到多传输

**网孔特点**：节点按二维网格排列（如 3×3 网孔：(0,0)-(0,1)-(0,2)；(1,0)-(1,1)-(1,2) 等），每个节点连接上下左右邻居（边缘节点连接较少），常用 X-Y 选路规则（先沿 X 轴、再沿 Y 轴传输）。

**CT 一到多传输过程**：
假设源节点 (0,0) 要向 (1,2)、(2,1) 发送数据：

- 源节点 (0,0) 生成消息头部（包含两个目标的坐标），立即启动传输。因目标在不同区域，采用 “分支转发” 策略：
  - 向 (1,2) 传输：按 X-Y 规则，先沿 X 轴发送头部到 (1,0)。节点 (1,0) 收到头部后，不等完整消息，立即判断需沿 Y 轴继续转发，边接收数据边传给 (1,1)，最终到达 (1,2)。
  - 向 (2,1) 传输：源节点同时（或稍晚）生成另一个头部（或同一头部标记多目标），先沿 Y 轴发送到 (0,1)。节点 (0,1) 收到头部后，判断需沿 X 轴转发到 (2,1)，边接收边传给 (1,1)，再到 (2,1)。
- 中间节点（如 (1,1)）若同时收到两个方向的消息，会通过 CT 的 “通道分离” 机制，分别处理不同目标的转发，避免冲突。

**特点**：利用网孔的二维路径特性，可向不同区域的目标 “并行分支” 转发，CT 的 “边收边传” 特性让每个分支的延迟都比 SF 低，适合多目标分散在不同行 / 列的场景。

### 3. 超立方（Hypercube）网络上的 CT 一到多传输

**超立方特点**：n 维超立方有 2ⁿ个节点，节点用 n 位二进制编号（如 3 维：000、001、010...111），编号仅差 1 位的节点直接相连，路径短（最多 n 步）。

**CT 一到多传输过程**：
假设 3 维超立方中，源节点 000 要向 010、101 发送数据：

- 源节点 000 生成消息头部（包含目标的二进制编号），无需等待完整数据，立即根据目标差异位转发：
  - 向 010 传输：000 与 010 仅第 2 位不同（0→1），直接将头部发送给相邻节点 010。010 收到头部后，发现自身是目标，立即开始接收并复制数据（同时无需继续转发）。
  - 向 101 传输：000 与 101 第 1 位（0→1）和第 3 位（0→1）不同，先将头部发送给第 1 位不同的相邻节点 100。100 收到头部后，不等完整消息，立即解析出还需修正第 3 位，边接收数据边转发给 101（100 与 101 仅第 3 位不同），101 收到后复制数据。
- 若目标更多（如向 001、011、101、111 发送），源节点 000 会先向所有直接相连的邻居（001、010、100）发送头部，每个邻居收到后，再根据自身与其他目标的差异位继续转发，形成 “扩散式” CT 传输（边收边传，快速覆盖所有目标）。

**特点**：超立方的 “短路径 + 多邻居” 特性与 CT 的 “低延迟” 完美匹配，多目标传输时能通过二进制位差异快速分支，延迟远低于环和网孔，是三种网络中 CT 效率最高的场景。



# 2.5.1 使用SF进行多到多播送

咱们用 “小区快递站送货” 的生活场景，把 “存储转发（SF）在三种网络上的多到多传输” 讲明白 ——“多到多” 就是**多个快递站（源节点）同时给多个住户（目标节点）送包裹**，“存储转发” 就是每个快递站 / 中转站必须 “先把包裹完整收下，再往下一站送”，不能只看个面单就转。

### 1. 环网络：小区里的 “环形快递路线”

#### 场景类比：

小区的快递路线是个圈（比如 1 号楼→2 号楼→3 号楼→4 号楼→1 号楼），每个楼只有 “左右两个邻居楼” 能直接送快递，没有其他小路。现在有两个快递站要送货：

- 快递站 A（1 号楼）要给 3 号楼送包裹；
- 快递站 B（2 号楼）要给 4 号楼送包裹。

#### SF 多到多传输过程：

1. **快递站 A 送货**：
   先把包裹完整交给 2 号楼（左邻居），2 号楼的快递点必须 “把包裹全收下、扫码确认没问题”（这就是存储转发的 “存”），再按环形路线交给 3 号楼（这就是 “转发”），3 号楼收下就完成了。
2. **快递站 B 送货**：
   同时把包裹交给 3 号楼（右邻居），3 号楼同样 “先收全、确认好”，再转发给 4 号楼，4 号楼收下完成。

#### 遇到 “堵车” 怎么办？

如果快递站 A 要给 4 号楼送，快递站 D 要给 2 号楼送，两条路线会在 3 号楼 “碰面”。这时 3 号楼的快递点会先收下 A 的包裹，转发完再收 D 的包裹 —— 按 “先到先处理” 的顺序排队，保证包裹不丢。

#### 特点：

路线固定像绕圈，简单但慢，要是小区（节点）多了，包裹要绕好多圈才能到。

### 2. 环绕网孔：小区里的 “网格快递路线”

#### 场景类比：

小区是 “棋盘格” 布局（比如 1 栋 1 单元→1 栋 2 单元→1 栋 3 单元；2 栋 1 单元→2 栋 2 单元→2 栋 3 单元），而且 “最左边单元能直接连最右边，最上边单元能直接连最下边”（比如 1 栋 3 单元右边是 1 栋 1 单元，2 栋 3 单元下边是 1 栋 3 单元）—— 相当于把棋盘卷成了 “甜甜圈”，没有 “尽头”。现在两个快递站送货：

- 快递站 C（1 栋 1 单元）要给 2 栋 3 单元送；
- 快递站 D（2 栋 2 单元）要给 1 栋 2 单元送。

#### SF 多到多传输过程：

1. 快递站 C 送货

   按 “先横后竖” 的规则（先沿 “栋数” 方向，再沿 “单元数” 方向）：

   - 先把包裹完整交给 2 栋 1 单元（横方向），2 栋 1 单元 “收全确认” 后，再沿单元方向交给 2 栋 2 单元，2 栋 2 单元 “收全确认” 后，交给 2 栋 3 单元（目标）。

2. **快递站 D 送货**：
   目标 1 栋 2 单元离得近，直接沿 “栋数” 方向（2 栋→1 栋），把包裹交给 1 栋 2 单元，对方 “收全确认” 就完成（因为环绕特性，不用绕远路）。

#### 遇到 “堵车” 怎么办？

如果两条路线都要经过 2 栋 2 单元，2 栋的快递点会先收下一条路线的包裹，转发完再处理另一条，同时还能选 “绕路”（比如横方向走左边还是右边），减少碰面的概率。

#### 特点：

像走棋盘，能选的路线多，比环形快，适合小区（节点）数量中等的情况。

### 3. 超立方网络：小区里的 “多维快递路线”

#### 场景类比：

小区的快递路线是 “多维的”—— 每个楼的编号是二进制（比如 000 楼、001 楼、010 楼…111 楼），只要两个楼的编号 “只差一个数字”（比如 000 和 001、010、100），就能直接送快递，相当于每个楼有 “多个邻居楼”（比如 000 楼能直接连 3 个楼）。现在两个快递站送货：

- 快递站 E（000 楼）要给 011 楼送；
- 快递站 F（100 楼）要给 110 楼送。

#### SF 多到多传输过程：

1. **快递站 E 送货**：
   000 楼和 011 楼的编号差 “后两位”（0→1，0→1），先把包裹完整交给 001 楼（改最后一位），001 楼 “收全确认” 后，再交给 011 楼（改中间一位），目标收下完成。
2. **快递站 F 送货**：
   100 楼和 110 楼只差 “中间一位”（0→1），直接把包裹交给 110 楼，对方 “收全确认” 就完成，一步到位。

#### 遇到 “堵车” 怎么办？

因为每个楼能连多个邻居，大部分包裹的路线都不重合（比如 E 走 000→001→011，F 走 100→110），很少堵车。就算偶尔碰面，快递点也会先收全一个再转另一个，不影响。

#### 特点：

路线像 “立体网”，邻居多、路径短，送得最快，适合小区（节点）特别多的情况（比如超级大社区）。



# 3.1.1 对称多处理机SMP结构特性

咱们可以把 SMP（对称多处理机）想象成一个 “共享厨房”，每个厨师（CPU）的地位、权限完全一样，一起用一套厨具（内存、硬盘、网卡等）做饭（处理任务）—— 这样类比下来，它的核心特性就很好懂了：

### 1. 所有 “厨师” 地位平等，没有 “主厨” 和 “帮厨”

平时咱们见的小厨房可能只有 1 个厨师（单 CPU），忙不过来；SMP 就像一个大厨房，雇了 2 个、4 个甚至 16 个厨师，但**没有高低之分**：

- 没人能命令别人：不管哪个厨师，都能直接拿冰箱里的菜（内存）、用燃气灶（I/O 设备），不用先问 “主厨” 同意；
- 任务随便分：比如来了 “炒 3 个菜” 的任务（用户请求），厨房经理（操作系统）可以把 “洗菜” 分给厨师 A，“切菜” 分给厨师 B，“炒菜” 分给厨师 C—— 谁有空就给谁，不用固定某个人干某件事。

### 2. 共用一套 “厨具”，不用各自带装备

所有厨师共用一个冰箱（共享内存）、一套锅碗瓢盆（I/O 设备），不用每人带个小冰箱：

- 拿东西不用 “传接力”：比如厨师 A 把切好的菜放进冰箱（写入内存），厨师 B 想用时，直接去冰箱拿就行（读取内存），不用 A 先把菜递给 B（区别于 “各带冰箱” 的分布式架构）；
- 得避免 “抢厨具打架”：比如两个厨师同时想拿同一口锅（访问同一块内存），得有规矩（硬件层面的 “缓存一致性协议”）—— 比如约定 “谁先拿到谁用，用完了喊一声让别人知道”，防止一个厨师把锅弄脏了（修改数据），另一个还拿脏锅用（读旧数据）。

### 3. 有 “公共通道”，但人多了可能挤

厨师们拿食材、用厨具，得走厨房中间的过道（系统总线 / 互连架构）：

- 早期小厨房：过道只有 1 条（共享总线），厨师多了会挤 —— 比如 3 个厨师同时想过过道，得排队（总线仲裁），慢；
- 现代大厨房：过道改成 “交叉路口”（交叉开关）或 “多条小路”（片上互连），比如厨师 A 走左边去冰箱，厨师 B 走右边去灶台，能同时走，不挤了（提升带宽）。

### 4. 没有 “专属工具”，谁用都行

比如厨房的洗碗机（网卡）、烤箱（硬盘），所有厨师都能开，没有 “这台洗碗机只能厨师 A 用” 的说法：

- 经理会安排：如果洗碗机洗完碗（网卡收到数据），经理会看哪个厨师有空，就让他去拿碗（处理数据），不会让一个厨师又炒菜又洗碗，忙不过来。

### 5. 经理（操作系统）得会 “协调”，不然乱套

虽然厨师地位平等、厨具共用，但得有个好经理（操作系统）管着：

- 别让厨师闲的闲、忙的忙：比如厨师 A 炒完菜没事干，经理就把厨师 B 手里的 “切菜” 任务分一些过去（负载均衡）；
- 别抢着干一件事：比如两个厨师都想改菜谱（修改同一段数据），经理会给一把 “钥匙”（锁机制），谁拿到钥匙谁改，改完再把钥匙给别人，避免菜谱改乱（竞态条件）。

### 6. 人不能太多，不然厨房转不开

虽然多雇厨师能快，但厨房就这么大（内存带宽、通道容量有限）：

- 最多雇多少？早期厨房可能最多 8 个厨师，再多就挤得没法干活；现在升级过道后，能雇到 32 个、64 个，但再往上（比如几百个），光协调 “谁用哪件厨具” 就耗半天，反而变慢了 —— 这就是 SMP “扩展性有限” 的原因。
- 有人请假怎么办？高端厨房会准备 “备用厨师”（CPU 热插拔），如果一个厨师突然生病（CPU 故障），能马上换一个顶上，不耽误做饭（容错性）。

### 最后总结：SMP 就像 “平等协作的共享厨房”

优点是 “灵活、高效”—— 厨师能随时帮同事，拿东西不用来回传，适合做 “需要快速响应” 的事（比如家里做饭、公司服务器处理日常请求）；
缺点是 “人不能太多”—— 厨房空间有限，人多了反而乱，所以超大规模的 “大食堂做饭”（比如超级计算机算天气），就不用这种模式了。

# 3.2 分布存储多计算机系统

要理解 MPP（大规模并行处理机）的 “并行能力” 为啥强，核心得看它的**结构特性**—— 这些设计就像为 “分工合作” 量身定制的规则，既保证每个节点能独立干活，又能让整个集群高效协同。咱们用 “拆解 + 对应功能” 的方式，把关键结构特性讲清楚：

### 一、核心结构：“多节点松散耦合”，拒绝 “单点依赖”

MPP 不是 “一台大电脑加多个配件”，而是**由几十到几千个 “独立节点” 组成的集群**，每个节点的地位平等，没有 “主节点指挥一切” 的强依赖（这叫 “松散耦合”），结构上就像 “一群各自带工具的工人，没有绝对的领导，只按规则分工”。

每个节点的 “标配” 包括：

- **计算单元**：独立的 CPU/GPU（比如每节点配 4-32 核 CPU），负责处理分配到的任务；
- **存储单元**：独立的本地硬盘 / SSD（不是共享一个大硬盘），只存自己要处理的数据；
- **内存单元**：独立的 DRAM，缓存自己任务的临时数据，不用和其他节点抢内存；
- **网络接口**：专门的高速网卡（比如 100G/400G 以太网、InfiniBand），用来和其他节点传数据。

👉 这个特性的好处：**不会 “一个节点坏了，整个集群瘫了”**。比如某节点故障，其他节点还能继续处理自己的任务，后续只需补算故障节点的部分，稳定性远超 “单台超级计算机”。

### 二、数据存储：“数据分片（Data Partitioning）”，避免 “大家抢同一堆数据”

这是 MPP 最关键的特性之一 ——**不会把所有数据存在一个地方，而是按规则 “切分” 到各个节点的本地存储里**，就像 “搬快递时，提前把 1000 箱快递分到 100 个工人的小推车上，每人只搬自己车上的”。

常见的 “分片规则” 有 3 种，对应不同场景：

1. **范围分片**：按数据范围切分（比如按用户 ID，1-10000 的用户数据存在节点 1，10001-20000 存在节点 2），适合需要按范围查询的场景（如 “查 ID 在 5000-8000 的用户消费”）；
2. **哈希分片**：用数据的某个字段（比如订单号）算 “哈希值”，按哈希值分配到节点（比如哈希值除以节点数取余数，余数 0 存节点 1，余数 1 存节点 2），适合随机查询场景（如 “查某笔订单详情”）；
3. **列表分片**：按具体值分组（比如 “北京用户存在节点 1，上海用户存在节点 2”），适合按固定类别统计的场景（如 “算各城市的订单量”）。

👉 这个特性的核心价值：**每个节点只处理自己存储的数据，不用跨节点读大量数据**（比如统计北京用户订单，只需要节点 1 干活），极大减少数据传输的 “时间浪费”，这是 MPP 快的关键。

### 三、任务处理：“任务并行（Task Parallelism）”，实现 “各干各的，不偷懒不抢活”

MPP 处理大任务时，会先通过**上层软件（比如分布式数据库、计算框架）把任务拆成 “小任务”**，再把小任务 “一对一” 分配给对应节点 —— 每个节点只执行自己的小任务，不会重复处理，也不会没事干。

举个具体例子：要统计全国 10 亿用户的平均消费额：

1. 任务拆解：先拆成 “计算每个省份用户的消费总和 + 用户数”（34 个省份对应 34 个小任务）；
2. 任务分配：把 “江苏省” 的小任务分给节点 A（节点 A 存着江苏用户数据），“广东省” 分给节点 B（节点 B 存广东数据）；
3. 并行计算：所有节点同时算自己的 “省份总和 + 用户数”；
4. 结果汇总：最后找一个节点（比如随机选一个），把 34 个节点的 “总和” 加起来、“用户数” 加起来，再算平均值。

👉 这个特性的关键：**任务拆解和数据分片是 “对齐” 的**—— 处理某个小任务的节点，正好存着这个任务需要的数据，不用 “跨节点借数据”，效率自然高。

### 四、节点通信：“高速专用网络”，解决 “组队干活的沟通效率问题”

虽然每个节点独立干活，但总有需要 “沟通” 的时候（比如汇总结果、传递中间数据）。如果用普通家用网络（比如千兆以太网），节点间传数据会很慢，就像 “工人之间用步行传消息，还没等传完，活都凉了”。

所以 MPP 会配**高速专用网络**，常见的有：

- **InfiniBand**：传输速度能到 400Gbps 以上，延迟极低（比如几十纳秒，相当于光走 10 米的时间），适合需要频繁传数据的场景；
- **RoCE（RDMA over Converged Ethernet）**：在普通以太网基础上实现 “远程直接内存访问”，不用经过 CPU 就能直接读写其他节点的内存，速度也能到 100Gbps 以上，成本比 InfiniBand 低。

👉 这个特性的作用：**让节点间的 “沟通” 像 “自己人说话” 一样快**，避免因为通信慢拖慢整个任务的进度。

### 五、容错机制：“分布式冗余”，不怕 “个别节点掉链子”

既然是 “一群节点组队”，就难免有节点出故障（比如硬件坏了、断电了）。MPP 的结构设计里，专门有 “容错” 能力，核心是 **“数据冗余 + 任务重试”**：

- **数据冗余**：重要数据会存多份（比如某份数据在节点 1 存一份，节点 2 再备份一份），如果节点 1 坏了，节点 2 能直接用备份数据继续处理，不用重新找数据；
- **任务重试**：如果某个节点的任务没完成（比如故障了），系统会自动把这个任务分配给其他空闲节点（用备份数据），重新计算，不会让整个任务卡住。

👉 这个特性的价值：**保证 “就算个别节点掉链子，整个活儿还能继续干”**，不会因为一个小故障导致几百个节点的集群停工。

### 总结：MPP 的结构特性，本质是 “为并行而生”

把上面的特性串起来看，MPP 的结构设计都是围绕 “高效并行” 展开的：

- 用 “多节点松散耦合” 保证 “不依赖单点，能扩能扛”；
- 用 “数据分片” 保证 “每个节点只干自己的活，不抢数据”；
- 用 “任务并行” 保证 “任务拆得匀，干活不偷懒”；
- 用 “高速网络” 保证 “节点沟通快，不拖后腿”；
- 用 “容错机制” 保证 “就算出点错，活儿能续上”。

正是这些结构特性，让 MPP 能轻松处理 “单台电脑扛不动” 的海量数据和复杂任务，成为大数据、高并发场景的 “主力军”。



# 3.3 分布共享存储计算机系统

分布共享存储计算机系统（Distributed Shared Memory, DSM）可以理解为 “既想享受分布式的灵活扩展，又想要共享内存的简单易用” 的混合体。它的核心特性就像 “一群邻居共用一个虚拟的大仓库”—— 物理上仓库被分到各家各户，但逻辑上大家觉得这是一个整体，不用费心记东西具体在谁家。

咱们从几个关键特性来拆解：

### 1. 物理分散，逻辑统一：“内存看起来是一块，实际藏在各个节点”

- **物理上**：内存被拆成多块，分散在不同的节点（可以理解为多台计算机）里，每个节点有自己的本地内存，就像邻居们各自家里有自己的储藏室。
- **逻辑上**：通过软件或硬件技术，让所有节点觉得它们在用 “同一块大内存”，程序不用关心数据具体存在哪个节点的内存里，就像大家都觉得东西存在 “小区公共仓库” 里，不用记是在 301 还是 502 的储藏室。

这种设计解决了一个痛点：传统分布式系统里，程序要手动处理数据在节点间的传递（比如发消息、存文件），而 DSM 让程序像用普通电脑的内存一样简单，大大降低了编程难度。

### 2. 访问透明：“用起来和单机内存一样，不用管数据在哪”

程序员写代码时，不需要专门写 “从节点 A 拿数据”“往节点 B 存数据” 的逻辑，直接用普通的内存读写指令（比如读变量、写数组）就行。

举个例子：如果程序要修改一个变量`x`，不管`x`实际存在节点 1 还是节点 2 的内存里，程序员只需要写`x = x + 1`，DSM 系统会自动搞定 “找到`x`在哪、加 1 后同步回去” 的过程，对程序员来说完全透明。

这种 “透明性” 是 DSM 的核心优势，让熟悉单机编程的人能快速上手分布式系统。

### 3. 数据一致性：“保证大家看到的内容一样，避免混乱”

既然多个节点可能同时访问同一块内存，就必须解决 “数据不一致” 的问题。比如节点 1 刚把`x`改成 10，节点 2 还以为`x`是 5，这就乱套了。

DSM 通过 “一致性协议” 来解决这个问题，常见的有：

- ** invalidate 协议 **：当节点 1 修改了`x`，就通知其他节点 “你们手里的`x`副本失效了，下次用得重新来我这拿”；
- ** update 协议 **：节点 1 修改`x`后，主动把新值发给其他存有`x`副本的节点，让大家同步更新。

就像小区仓库有个管理员：如果 301 改了仓库里的东西，要么告诉其他邻居 “你们之前记的版本作废了”，要么直接把新东西送到邻居家，保证大家手里的信息一致。

### 4. 扩展性较好，但有 “距离感”：“节点越多，通信成本可能越高”

DSM 比传统的共享内存多处理器（SMP）扩展性好 ——SMP 就像 “一个大房子里塞很多 CPU，共用一块内存”，内存总线会成为瓶颈，塞不了太多 CPU；而 DSM 是 “多个小房子（节点）连起来”，能轻松加更多节点。

但它也有局限：数据实际存在某个节点的本地内存里，其他节点访问时，本质上还是要通过网络传输（虽然程序员感觉不到）。节点越多、距离越远（比如跨机房），数据传输速度就越慢，就像邻居之间拿东西快，跨小区拿东西就慢。

所以 DSM 更适合节点数量不算特别多（比如几十到几百个）、节点间距离近的场景，比如中小型数据中心的集群。

### 5. 容错性：“个别节点坏了，系统还能转”

因为内存分散在多个节点，就算某个节点故障，其他节点的内存和数据还在，系统可以通过冗余机制（比如重要数据存多个副本）恢复故障节点的数据，不会像单机那样 “内存坏了整个系统崩掉”。

就像小区某户人家储藏室坏了，只要其他人家有备份，仓库里的东西还能正常用。

总结一下，分布共享存储系统的核心是 “用分布式的硬件，模拟出共享内存的效果”—— 既保留了分布式系统易于扩展的优点，又降低了编程门槛，让程序能像操作单机内存一样处理分布式数据。但它本质上还是要处理节点间的通信和数据一致性问题，这也是它和纯共享内存、纯分布式系统的关键区别。

# 3.4.1大规模并行处理系统MPP，机群SP2





# 3.4.2 工作站机群COW

工作站机群 COW（Cluster of Workstations）是上世纪 90 年代兴起的松散耦合并行计算系统，简单说就是把多台工作站、PC 甚至小型服务器，通过普通局域网连起来，再搭配专用并行软件，组成一个能协同工作的 “虚拟超级计算机”。它不像传统超级计算机那样造价高昂，却能满足中小规模的大规模计算需求，在科研院校和中小企业中应用广泛，以下是其核心特性和相关细节介绍：

1. 硬件结构：普通组件拼出 “计算团队”

   - **独立完整的计算节点**：每个节点都是一台完整的计算机，要么是去掉显示器、键盘的 “无头工作站”，要么是普通 PC 或 SMP 服务器。每个节点都带本地磁盘，还有自己的 CPU、内存，就像团队里每个成员都有全套工具，能独立干活。这和 MPP 节点常无本地磁盘的设计差别很大。
   - **低成本的互连网络**：节点之间靠以太网、FDDI 这类商品化局域网连接，部分商用款会用定制网络，但整体成本远低于 MPP 的专用高速紧耦合网络。而且节点的网络接口是和 I/O 总线松耦合的，不像 MPP 那样和存储总线紧耦合，硬件搭建灵活且性价比高。

2. 软件系统：靠 “协同层” 实现统一作战

   - **各节点有独立完整系统**：每个节点都装着完整的操作系统，比如工作站常用的 UNIX。这和 MPP 节点常只装操作系统微核不同，就算某个节点系统出问题，也不会影响其他节点正常运行。
   - **专用软件支撑并行计算**：会额外加一层软件来实现 “协同”，比如支持单一系统映像，让用户用起来像操作一台电脑，而非多台；还靠 PVM、MPI 这类消息传递软件，让节点间能传递数据和任务指令。比如处理一个大的数据分析任务时，软件会把任务拆分给不同节点，计算完再汇总结果。

3. 核心优势：适配中小用户的高性价比之选

   - **成本低、风险小**：不用买专用高端硬件，用现成的工作站或 PC 就能搭建，对资金有限的科研院校和中小企业很友好，就算后续调整也不用大幅投入。
   - **扩展性和灵活性强**：想提升算力，直接加节点就行，节点还能是同构也能是异构的；而且既能处理并行任务，单个节点也能单独供用户做普通计算，不会浪费资源。
   - **软件兼容性好**：能兼容工作站上的原有软件，不用为了并行计算重新开发大量程序，大大降低了使用门槛，原有软件财富能直接继承。

4. 明显短板：并行计算的 “小麻烦”

   它也有不少待解决的问题。一方面并行程序设计难度高，编程者得自己处理节点间的消息交换和任务协调，而且 MPI 这类工具的库函数繁多，掌握起来很费力；另一方面通信带宽较低，节点间数据传输的开销大，还容易出现负载不均衡的情况，某个节点忙到卡顿，其他节点可能还处于空闲状态。另外它容错性较差，MPI 早期甚至不支持节点动态增减，一旦某个节点故障，整个计算任务很难恢复。

如今随着技术发展，MPP 和 COW 的界限越来越模糊，不少 MPP 也借鉴了 COW 的低成本节点设计，但 COW 凭借高性价比和灵活部署的特点，至今仍是并行计算领域的重要形态。像 Berkeley NOW、Alpha Farm 等都是 COW 结构的典型代表。



# 4.1并行计算机的一些基本性能指标

## 4.1.1CPU和存储器的某些基本性能指标

### 一、工作负载（Workload）：“计算机要干的活有多少”

工作负载是指并行计算机需要处理的**任务总量或数据总量**，本质是衡量 “计算需求的大小”，就像给计算机的 “工作量清单”。

- **核心定义**：通常用 “计算操作数”（比如执行多少次加法、乘法）或 “数据量”（比如处理多少 GB/TB 的数据）来量化，也可以是具体任务类型（如科学计算、数据挖掘）。
- 关键分类
  1. **计算密集型**：任务以大量数学运算为主，数据量不大但计算步骤多（比如天气预报的流体力学模拟）。
  2. **数据密集型**：任务需要处理海量数据，计算步骤相对简单，但数据读取 / 传输占比高（比如电商平台的用户行为分析）。
- **为什么重要**：工作负载决定了 “并行计算机需要分配多少资源”。比如计算密集型任务要多配 CPU/GPU，数据密集型任务要优化存储器和带宽，选对负载类型才能精准评估性能。

### 二、并行执行时间（Parallel Execution Time）：“干完活要花多久”

并行执行时间是指并行计算机从**开始处理任务到所有节点完成计算、汇总出结果**的总时间，是衡量 “计算效率” 最直接的指标。

- **核心定义**：通常用秒（s）、毫秒（ms）甚至微秒（μs）计量，需要和 “串行执行时间”（单台计算机处理同一任务的时间）对比，才能体现并行优势。
- 关键构成（3 个部分）：
  1. **计算时间**：各节点实际处理分配到的小任务的时间。
  2. **通信时间**：节点间传递数据（如中间结果、汇总信息）的时间，是并行计算的主要 “耗时瓶颈”。
  3. **同步时间**：节点间等待彼此进度的时间（比如某节点算完了，要等其他节点同步后再进入下一步）。
- **为什么重要**：并行的核心目标是 “提速”，如果并行执行时间比串行还长（比如通信 / 同步时间占比太高），就失去了并行的意义。实际评估中，会用 “加速比”（串行时间 / 并行时间）来更直观地体现性能提升。

### 三、存储器层次结构（Memory Hierarchy）：“数据存在不同‘仓库’里”

并行计算机的存储器不是 “一块大内存”，而是按 “速度快慢、容量大小、成本高低” 分成多层的结构，就像家里的 “冰箱（快取常用菜）、厨房柜（存常用调料）、储藏室（存大量干货）”，目的是平衡 “速度” 和 “成本”。

- 核心结构（从快到慢、容量从小到大）
  1. **寄存器（Register）**：在 CPU 内部，速度最快（纳秒级）、容量最小（KB 级），存当前正在计算的指令和数据。
  2. **高速缓存（Cache）**：分 L1/L2/L3（CPU 内或附近），速度次之（十纳秒级）、容量较小（MB 级），存近期可能用到的数据，减少 CPU 访问内存的次数。
  3. **主内存（Main Memory）**：即通常说的 “内存”（DRAM），速度中等（百纳秒级）、容量较大（GB / 数十 GB 级），存当前任务的所有数据。
  4. **辅助存储（Secondary Storage）**：如硬盘（HDD）、固态硬盘（SSD），速度慢（毫秒级）、容量大（TB/PB 级），存长期不用的海量数据。
- **为什么重要**：并行计算中，数据频繁在各层存储器间移动，层次结构设计直接影响 “数据读取速度”。比如如果常用数据都能存在 Cache 里，CPU 就不用频繁等内存，计算效率会大幅提升；反之，若频繁读硬盘，会严重拖慢整体速度。

### 四、存储器带宽估计（Memory Bandwidth Estimation）：“数据传输的‘高速公路’有多宽”

存储器带宽是指**单位时间内存储器与 CPU / 其他部件之间能传输的数据量**，相当于数据传输的 “高速公路车道数”，带宽越高，单位时间传的数据越多，通常用 “GB/s”（千兆字节 / 秒）计量。

- **核心定义**：分 “峰值带宽”（理论最大传输速度，由硬件规格决定）和 “实际带宽”（实际使用中能达到的速度，受软件、数据访问模式影响），估计时需结合两者。
- 关键影响因素
  1. **硬件设计**：如内存总线宽度（64 位比 32 位带宽高）、内存类型（DDR5 比 DDR4 带宽高）、互连网络速度（如 MPP 的高速网络比 COW 的普通以太网带宽高）。
  2. **数据访问模式**：“连续访问”（如读取数组的连续元素）比 “随机访问”（如跳着读分散的数据）带宽更高，因为连续访问能利用存储器的预取机制，减少等待时间。
  3. **并行度**：多节点同时访问存储器时，总带宽会受 “总线拥堵” 影响，比如 10 个节点同时读内存，实际总带宽可能低于 10 倍单节点带宽。
- **为什么重要**：并行计算中，大量数据需要在节点间、存储器各层间传输，带宽不足会导致 “数据传得比算得慢”，CPU 就算再快也得等数据，形成 “带宽瓶颈”。比如数据密集型任务（如大数据分析），若存储器带宽不够，并行优势会完全被抵消。

### 总结：四个指标的关联的

这四个指标相互影响、共同决定并行计算机的性能：

- **工作负载**决定 “任务规模”，是评估的前提；
- **并行执行时间**是最终的 “效率结果”，受计算、通信、同步时间影响；
- **存储器层次结构**和**存储器带宽**则是 “数据供给能力” 的关键，直接影响计算和通信时间 —— 只有数据能快速 “存得下、传得快”，才能让并行计算真正 “跑得起来、跑得更快”。

## 4.1.2 通信开销

热土豆法是并行计算中一种多节点通信开销的实测方法，Hockney 表达式是对点到点通信开销进行建模的经典公式，二者分别从实测和理论建模角度解决点到点通信开销的计算问题，以下是详细介绍：

1. 热土豆法（Hot - Potato）

   该方法又称救火队法，是在乒乓法基础上扩展而来的多节点通信开销测量方法。乒乓法仅适用于两个节点间的开销测量，而热土豆法可实现多个节点组成的网络中点到点通信开销的计算，具体细节如下：

   - **核心原理**：核心思路和乒乓法一致，都是通过消息的循环传输并结合总耗时来反推单次点到点通信的开销。它就像传递烫手的土豆一样，让消息在节点间快速传递，不做额外停留，以此排除队列等待等额外因素对通信时间的干扰。
   - **计算步骤**：首先构建一条节点传输链路，如节点 0→节点 1→节点 2→…→节点 n - 1→节点 0；接着从节点 0 发送长度为 m 字节的消息，消息依次在后续节点间快速传递，最终由节点 n - 1 传回节点 0；最后记录整个传输过程的总时间 T，由于消息完成了 n 次点对点的传输，因此单次点到点通信开销 t = T /n。
   - **特点**：测量过程贴合实际通信场景，能反映多节点传输中的真实开销情况，操作简单且易实现。但该方法可能受节点硬件性能差异、链路临时拥堵等突发因素影响，多次测量结果可能存在小幅波动，通常需多次测量取平均值来提升准确性。

2. Hockney 表达式

   该表达式源自 Hockney 模型（也叫 α - β 模型），是并行计算领域描述点到点通信开销的经典线性模型，能通过关键参数快速估算通信开销，具体内容如下：

   - 核心形式

     ：其基础表达式有两种常见表述，本质等价。一种表述为

     *t*(*m*)=*t*0+*r*∞*m*

     ，另一种表述为

     *t*(*m*)=*α*+*m*×*β*

     。各参数含义如下：

     | 参数         | 含义                                                         |
     | ------------ | ------------------------------------------------------------ |
     | *t*(*m*)     | 长度为 m 字节消息的点到点通信总开销（单位通常为微秒）        |
     | m            | 消息长度（单位为字节）                                       |
     | *t*0（即 α） | 通信启动时间，指完成通信初始化、协议协商等准备工作的固定耗时，与消息长度无关 |
     | *r*∞         | 渐近带宽，指传输无限长消息时网络能达到的稳定通信速率（单位通常为 MB/s） |
     | β            | 消息传输的单位长度耗时，与渐近带宽成反比，即*β*=*r*∞1        |

   - **补充关联参数**：该模型还衍生出两个辅助参数用于更全面的性能评估。一是半峰值长度*m*1/2，即达到一半渐近带宽所需的消息长度，满足*t*0=*r*∞*m*1/2；二是特定性能*π*0，代表短消息带宽，与启动时间的关系为*t*0=*π*01。

   - **特点**：公式简洁直观，仅通过启动时间和渐近带宽两个核心参数就能快速估算不同长度消息的通信开销，广泛用于并行算法的性能预测和网络架构的优化设计。但该模型是理想化的线性假设，未考虑网络拥堵、链路竞争等实际干扰因素，估算结果与真实场景可能存在一定偏差。

     

# 4.2加速比定律



加速比就是并行计算相对于串行计算的 “提速倍数”，核心是用**串行执行时间**除以**并行执行时间**得到的比值，用来衡量并行系统的性能提升效果。

简单理解，比如一个任务单独用 1 台电脑（串行）要 10 分钟，用 10 台电脑（并行）只要 2 分钟，那加速比就是 10÷2=5，意味着并行后速度快了 5 倍。

这三个定律从不同维度揭示了并行计算的加速比极限，是评估并行系统性能的核心理论。其中 Amdahl 定律关注固定问题规模下的加速比上限，Gustafson 定律聚焦问题规模可扩展时的加速潜力，Sun 和 Ni 定律则进一步考虑了通信开销的实际影响。

### 1. Amdahl 定律（Amdahl's Law）

Amdahl 定律是并行计算的基础定律，核心是**固定问题总规模**时，并行加速比受限于串行部分的比例。

- #### 核心公式

  ![image-20251105213039453](C:\Users\86132\AppData\Roaming\Typora\typora-user-images\image-20251105213039453.png)

  #### 变量说明

  ![image-20251105213115796](C:\Users\86132\AppData\Roaming\Typora\typora-user-images\image-20251105213115796.png)

- **关键结论**

  1. 即使处理器数量 p 趋近于无穷大，加速比上限也仅为 1−*f*1，串行部分是核心瓶颈。
  2. 若串行比例为 10%（f=0.9），理论加速比上限仅为 10，再多处理器也无法突破。

### 2. Gustafson 定律（Gustafson's Law）

Gustafson 定律针对 Amdahl 定律的局限性，提出**固定并行执行时间**的场景，允许问题规模随处理器数量增加而扩展。

- ![image-20251105213150482](C:\Users\86132\AppData\Roaming\Typora\typora-user-images\image-20251105213150482.png)

  其中参数定义与 Amdahl 定律一致。

- **关键结论**

  1. 当问题规模随处理器数量成比例扩展时，加速比可接近处理器数量 p，突破了 Amdahl 定律的上限。
  2. 若可并行比例 f=0.9，使用 100 个处理器时，加速比约为 99.1，远高于 Amdahl 定律的 10。

### 3. Sun 和 Ni 定律（Sun & Ni's Law）

Sun 和 Ni 定律在 Amdahl 定律基础上，**加入了并行通信开销**，更贴近实际并行系统的性能表现。

- ![image-20251105213255609](C:\Users\86132\AppData\Roaming\Typora\typora-user-images\image-20251105213255609.png)

  其中：

  - 前两项与 Amdahl 定律一致。
  - *h*(*p*)：p 个处理器间的总通信开销（如数据传输、同步耗时）。
  - *W*：串行执行时的总计算工作量（用于将通信开销归一化到计算时间维度）。

- **关键结论**

  1. 通信开销会显著降低实际加速比，尤其是处理器数量 p 增加时，通信开销可能成为新瓶颈。
  2. 若通信开销随 p 增长过快（如 h (p)∝p²），即使处理器数量增加，加速比也可能不升反降。

### 三大定律核心差异对比

| 对比维度         | Amdahl 定律            | Gustafson 定律           | Sun 和 Ni 定律                |
| ---------------- | ---------------------- | ------------------------ | :---------------------------- |
| **问题规模假设** | 固定总问题规模         | 随处理器数量扩展问题规模 | 固定总问题规模                |
| **核心瓶颈**     | 串行执行部分           | 无（理论加速比趋近 p）   | 串行部分 + 通信开销           |
| **加速比上限**   | 有限（1/(1−*f*)）      | 无限（趋近处理器数量 p） | 有限（受串行 + 通信双重限制） |
| **适用场景**     | 小规模问题、串行比例高 | 大规模问题、可扩展任务   | 实际并行系统（含通信开销）    |
| **理想 / 实际**  | 理想模型（无通信开销） | 理想模型（无通信开销）   | 更贴近实际（含通信开销）      |

# 4.3可扩放性评测标准

并行计算的可扩放性，说白了就是 “电脑（处理器）越多，干活能力能不能跟着涨” 的本事 —— 比如 10 台电脑能比 1 台快 10 倍，100 台能快 100 倍，就说明可扩放性好；要是 100 台只快 50 倍，那就是可扩放性差。

下面三个度量标准，其实就是三种 “检验可扩放性好不好” 的通俗方法：

------

### 1. 等效率：“人多了，别偷懒”

核心就是**不管加多少人（处理器），大家的干活效率得保持一样**。

比如 10 个人干 100 份活，每人干 10 份，1 小时完成（效率 100%）；现在加到 20 人，得让这 20 人还是 1 小时完成，而且每人依然干 10 份活 —— 这就需要把总活量加到 200 份。

如果加人后，只需要稍微多加点活（比如 20 人干 210 份）就能保持效率，说明可扩放性好；要是得加巨多活（比如 20 人干 500 份）才能不偷懒，那就是可扩放性差。

------

### 2. 等速度：“每个人的干活速度别降”

核心是**不管加多少人，每个人每秒能干的活，得跟原来一样多**。

比如 1 个人 1 小时干 10 份活（速度 10 份 / 小时）；加到 10 人后，总活量得变成 100 份，而且 10 人还是 1 小时干完 —— 这样每人速度还是 10 份 / 小时。

如果加人后，稍微加点头就能让每个人速度不变，说明可扩放性好；要是加了人，每个人速度反而变慢（比如 10 人 1 小时只干 80 份，每人速度 8 份 / 小时），那可扩放性就不行。

------

### 3. 平均延迟：“别光干活，沟通别耽误事”

核心是**人多了，大家互相传话（通信）的时间别太长**。

比如 10 人干活，每人平均花 1 分钟沟通；加到 100 人后，要是每人沟通时间还是 1 分钟左右，说明沟通没拖后腿，可扩放性好；要是 100 人时，每人沟通要花 10 分钟，那光沟通就占了大部分时间，活根本干不完，可扩放性自然差。

------

简单总结下：

- 等效率看 “人多了会不会偷懒”，
- 等速度看 “人多了个人速度降没降”，
- 平均延迟看 “人多了沟通会不会拖慢”。

